---
title: "data-analysis-rep"
output: html_document
date: "2024-04-13"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Differential Phasing between Circadian Clocks in the Brain and Peripheral Organs in Humans
## Jacob J. Hughey and Atul J. Butte 2016
### Background 
All organisms have synchronized themselves with the natural light-dark cycles of the earth, created by the rising and falling of the sun. They are able to achieve this through their circadian clock, an approximately 24-hour molecular oscillator, which regulates physiological rhythms such as sleep/wake, hunger, metabolism, as well as mood.  

While there is evidence to suggest that positive moods are under circadian control,  results have been inconsistent for negative moods. 

*The goal of this study is to assess time-of-day differences in  positive and negative moods by extracting indicators of mood from Twitter data to overcome the sampling frequency, sampling size, and recall bias of questionnaire-based studies.*

### Data Set Collection
The authors collected Twitter data from the 54 largest towns and cities in the United Kingdom from January 1, 2010 - October 31, 2014. Contents were collected every 10 minutes, each time retrieving the 100 most recent tweets and then removing any duplicates from the previous iterations. This resulted in 800 million tweets and 33,576 time points. 

### Data Analysis 
Individual tweets were aggregated by hour, so to have 24 data points per day in each city. Authors removed any standardized greeting messages in specific seasons as follows: tweets containing the word happy, merry, good, lovely, nice, great, or wonderful followed by christmas, halloween, valentine, easter, new year, mother day, father day, and their variants.

Authors standardise the time series of each word individually so that each day receives zero mean and unit standard deviation. This procedure incorporates their observation, directly rescaling the time series of a word in terms of its intraday z-scores. 

Authors first compute the Fourier decompoisition of the 33,576 time point series of averages across the words in a given list, each standardised in the 4 years, and extract the largest frequences response that corresponds with a sine wave of period under a year. The significance of the percentage of variance explained by the sine oscillation is then tested in a Monte Carlo setting using 100,000 perumutations of the original series. 

#### Figure Analysis: Figure 1
For Dzogang et al.  **Figure 1**, they look at the variation of word volume within the 24-hour cycle, with a 99% confidence interval. In order to achieve this, author's averaged the hourly word volume. I then assume they calculated their 99% confidence intervals from student t-test. They have the y-axis representing that average volume per hour in the top 20,000 most frequent words across the 4 years. Although not labeled, the x-axis represents hours, starting at 6 AM in the morning. 
- This includes inferential statistics (99% confidence intervals)

```{r, Fig 1}
# Start by loading in libraries
library(tidyverse)
library(dplyr)
library(ggplot2)
library(skimr)
library(reshape2)
library(zoo)
library(chron)


# Grab data
f <- "https://raw.githubusercontent.com/kierstenbell/data-analysis-rep/main/data/vol.csv"
d <- read.csv(f, sep = " ")
colnames(d) <- c("date", "time", "wordfreq") #creating column names to make data easier to work with

# Let's take a look out our data
head(d)
skim(d)

# There is date (01-01-2010 - 31-10-2014), time (00:00:00-midnight - 23:00:00-11 PM), and word frequency. 
# To generate the figure, we will need to average each hour across all the days and generate 99% confidence intervals

# Get average words per hour (AvgFrequency) and generate 99% confidence intervals (alpha = 0.01)
alpha = 0.01

d.Avg <- summarize(.data = d, 
                      AvgFrequency = mean(wordfreq),
                      lower_ci = mean(wordfreq) + qnorm(1 - (alpha/2))* sqrt(var(wordfreq)/length(wordfreq)), # lower 99% CI
                      upper_ci = mean(wordfreq) - qnorm(1 - (alpha/2))* sqrt(var(wordfreq)/length(wordfreq)), # upper 99% CI
                      .by = time)

# Author's plot starts at 6 (AM), current data will have the plot starting at midnight.
# Mutate the data so that it can start at a similar time frame
# There's probably a better way to do this but I'm creating the variable "x" so that 6 AM will be first on the chart, followed by 7 AM will be next, until 5 AM is the last point on the graph. It is possible to shift the time using as.POSIXct (see Figure 2) but I cannot get it to work.
d.Avg <- d.Avg %>%
 mutate(x = case_when(time =="00:00:00" ~ "19",
                      time =="01:00:00" ~ "20",
                      time =="02:00:00" ~ "21",
                      time =="03:00:00" ~ "22",
                      time =="04:00:00" ~ "23",
                      time =="05:00:00" ~ "24",
                      time =="06:00:00" ~ "01",
                      time =="07:00:00" ~ "02",
                      time =="08:00:00" ~ "03",
                      time =="09:00:00" ~ "04",
                      time =="10:00:00" ~ "05",
                      time =="11:00:00" ~ "06",
                      time =="12:00:00" ~ "07",
                      time =="13:00:00" ~ "08",
                      time =="14:00:00" ~ "09",
                      time =="15:00:00" ~ "10",
                      time =="16:00:00" ~ "11",
                      time =="17:00:00" ~ "12",
                      time =="18:00:00" ~ "13",
                      time =="19:00:00" ~ "14",
                      time =="20:00:00" ~ "15",
                      time =="21:00:00" ~ "16",
                      time =="22:00:00" ~ "17",
                      time =="23:00:00" ~ "18"))

                         
# Let's generate the plot
fig.1 <- ggplot(d.Avg, aes(x = x, y = AvgFrequency, group = 1)) + # group = 1 needed for geom_line()
      geom_point() + # Add points
      geom_line() + # Connect the points, **need group = 1 in aes 
      geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.4, fill = "blue") +  # Add shaded area for confidence interval; lower_ci and upper_ci calculated above
      scale_x_discrete(labels = c("6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "0", "1", "2", "3", "4", "5")) + # Manually change the x-axis labels to the correct times
      scale_y_continuous(limits = c(0.5e+05, 3e+05)) +  # Adjust y-axis limits
      labs(x = "Hour", y = "Average Word Frequency") +  # Label the axes
      ggtitle("Figure 1 Replicate")  # Add a title

# Compare generated plot with original figure
# Grab Fig 1 Url from Github
AuthorsFig1_url <- "https://github.com/kierstenbell/data-analysis-rep/blob/main/Figures/Figure%201.jpg?raw=true"  

# Output plot to a temporary files in order to use knitr
plot_file <- tempfile(fileext = ".png")
ggsave(plot_file, plot = fig.1, width = 5, height = 4, dpi = 300)

# Compare my figure (top) to author's figures (bottom)
knitr::include_graphics(c(plot_file, AuthorsFig1_url))
```

Figure 1 was able to be mostly successfully replicated. The authors did not report any physical numbers, so it is unclear whether this is an *exact* match; however, it is graphically very similar.  The data was easy to access (unlike Hughey & Butts 2016 paper!); however, the authors did not provide a lot of detail about how they conducted this particular analysis. I made some assumptions, but it appears as though those assumptions were correct. 

Based on this data, the UK population does not Tweet very much between 2 AM - 9 AM. Peak tweeting, as defined characterized by use of 20,000 most frequent words, occurs between 9 - 11 PM.  

#### Figure Analysis: Figure 2
For Dzogang et al.  **Figure 2**, they are showing that their methods on a group of example words (e.g., "sunrise" or "bus") are over-expressed and under-expressed at the expected times. In this case, "sunrise" and "bus" are overexpressed at 8 AM with some slight overexpression in the afternoon.  To demonstrate this, they use heatmaps that show the relative expression of a word in different months in each row and its relative expression during the 24-hours in each column. In other words, the y-axis is months and the x-axis is 24 hours. The authors now choose to start their x-axis at 0 (i.e., midnight), unlike Figure 1, for unknown reasons. The heatmaps are smoothed using cubic interpolation with secren interpolated points between each sampled value. Contours indicate sunrise and sunset in local time in London, United Kingdom. 

This analysis is much more difficult and **can not be completed**.  In all their 9,000 csv files, there is not one called "sunrise" or "bus". Authors also do not describe what software they used or how they actually collected this data, so I am unable to attempt to glean Twitter data for these words. Authors also did not include sunrise/sunset data.  In order to attempt this, I will use one of the files they have included called "sunset". So although it will not be the same as what they have provided, it will be an attempt to mimic what they did. 

I collected the data from [here]("https://www.timeanddate.com/sun/uk/london?month=10&year=2010") that included that date and time of sunset in the UK. This information is uploaded to github. Authors should have included this.  

Figures 3 - 11 are very similar. It is not clear exactly what words were considered for the plots. 

```{r, Figure 2}
# Load data set titled "sunset", where `sunset' was extracted based on the Linguistic Inquiry and Word Count.
f <- "https://raw.githubusercontent.com/kierstenbell/data-analysis-rep/main/data/sunset.csv"
d_sunset <- read.csv(f, sep = " ")
colnames(d_sunset) <- c("date", "time", "wordfreq") #creating column names to make data easier to work with

# Check to make sure data looks ok
# Note: certain hours are missing - this indicates that the word "sunset" was not tweeted
head(d_sunset)

# Preprocess the data
# as.POSIXct converts character class to the POSIXlt class which allows manipulation of date/time
# This will allow us to easily extract month data
d_sunset$date_time <- as.POSIXct(paste(d_sunset$date, d_sunset$time), format = "%d-%m-%Y %H:%M:%OS")
d_sunset$hour_of_day <- as.numeric(format(d_sunset$date_time, "%H"))
d_sunset$month <- as.numeric(format(d_sunset$date_time, "%m"))
d_sunset$year <- as.character(format(d_sunset$date_time, "%Y"))

# if we skim this data set, we will see that we have different classes
skim(d_sunset)

# Average the data across the 4 years
# Since we are averaging across the years, but needed to keep month and hour, use aggregate() instead of groupby()
d_sunset.Avg <- aggregate(x=d_sunset$wordfreq, 
                          by=list(d_sunset$hour_of_day,d_sunset$month), FUN=mean)

colnames(d_sunset.Avg) <- c("hour", "month", "wordfreq")
skim(d_sunset.Avg)

# Create a matrix with hours of the day as columns (0 - 23) and months (1-12) as rows
sunset_df <- dcast(d_sunset.Avg, month ~ hour, value.var = "wordfreq")
sunset_matrix <- as.matrix(sunset_df) # matrix needed for heatmap in ggplot
sunset <- melt(sunset_matrix) # needed for heatmap in ggplot
head(sunset) # checking to make sure there are correct values

# Created my own csv sheet with sunset times in London, UK from 01-01-2010 - 31-10-2014
# Load data set titled "SunsetTimes"
g <- "https://raw.githubusercontent.com/kierstenbell/data-analysis-rep/main/data/SunsetTimes.csv"
d_sunset.times <- read_csv(g, col_names = TRUE)
head(d_sunset.times)

# Process Data by converting characters to as.posixct so we can average the time 
d_sunset.times$date_time <- as.POSIXct(paste(d_sunset.times$Date, d_sunset.times$Sunset), format = "%Y-%m-%d %H:%M")

# Extract only the time ; if change to as.numeric throws an error 
d_sunset.times$time <- format(d_sunset.times$date_time, format="%H:%M:%S")

# Extract month
d_sunset.times$month <- as.character(format(d_sunset.times$date_time, "%m"))

# Extract year
d_sunset.times$year <- as.character(format(d_sunset.times$date_time, "%Y"))

# Get average sunset time 
library(chron)
d_sunset.times.average <- d_sunset.times %>%
  group_by(month) %>%
  reframe(AvgSunSet = mean(chron::times(time)))



# Perform cubic interpolation
# Authors state they want "cubic interpolation" with 7 interpolated points, which would be a spline
# Authors had  interpolated points between each sample value so xout = 7

# Sunset times

# Create a heatmap
heatmap <- ggplot(data = sunset, aes(x = Var2, y = Var1, fill = value)) +
                  geom_tile() +
                  scale_fill_gradientn(colours = c("green", "white", "red")) +
                  coord_fixed(ratio = 1) +  # Adjust ratio_value to lengthen or shorten the plot
                  labs(x = "Hour of Day", y = "Month", fill = "Word Frequency") +
                  scale_y_continuous(breaks = seq(1, 12, by = 1)) +
                  theme(axis.text.x = element_text(angle = 45, hjust = 1))

complete_plot <- heatmap +
  geom_point(data = d_sunset.times.average, aes(x = AvgSunSet, y = month), color = "black", size = 100)
      
```
