---
title: "data-analysis-rep"
output: html_document
date: "2024-04-13"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Differential Phasing between Circadian Clocks in the Brain and Peripheral Organs in Humans
## Jacob J. Hughey and Atul J. Butte 2016
### Background 
All organisms have synchronized themselves with the natural light-dark cycles of the earth, created by the rising and falling of the sun. They are able to achieve this through their circadian clock, an approximately 24-hour molecular oscillator, which regulates physiological rhythms such as sleep/wake, hunger, metabolism, as well as mood.  

While there is evidence to suggest that positive moods are under circadian control,  results have been inconsistent for negative moods. 

*The goal of this study is to assess time-of-day differences in  positive and negative moods by extracting indicators of mood from Twitter data to overcome the sampling frequency, sampling size, and recall bias of questionnaire-based studies.*

### Data Set Collection
The authors collected Twitter data from the 54 largest towns and cities in the United Kingdom from January 1, 2010 - October 31, 2014. Contents were collected every 10 minutes, each time retrieving the 100 most recent tweets and then removing any duplicates from the previous iterations. This resulted in 800 million tweets and 33,576 time points. 

### Data Analysis 
Individual tweets were aggregated by hour, so to have 24 data points per day in each city. Authors removed any standardized greeting messages in specific seasons as follows: tweets containing the word happy, merry, good, lovely, nice, great, or wonderful followed by christmas, halloween, valentine, easter, new year, mother day, father day, and their variants.

Authors standardize the time series of each word individually so that each day receives zero mean and unit standard deviation. This procedure incorporates their observation, directly rescaling the time series of a word in terms of its intraday z-scores. 

Authors first compute the Fourier decomposition of the 33,576 time point series of averages across the words in a given list, each standardized in the 4 years, and extract the largest frequency response that corresponds with a sine wave of period under a year. The significance of the percentage of variance explained by the sine oscillation is then tested in a Monte Carlo setting using 100,000 perumutations of the original series. 

The data that **I will duplicate** look at variation of word volume, both as a whole and for an individual word. Figures 1 and 2 are much less complicated than Figures 3-11. In short, Figure 1 looks at the hourly tweeting rates over the course of the month, averaged over 4 years. Author's construct a 99% confidence interval to look at the variation of word volume. Figure 2 looks at the hourly tweeting rates for the word "sunset" over the course of each month, averaged over 4 years. This is visualized through a heatmap. 

### Results/Conclusions
I will not be personally walking through the Figures that show mood as authors (1) do not provide enough information for their analysis despite the 9,000 data files and (2) analysis are rather complicated with Fourier decomposition and variance explained by sine oscillations. However, this paper is interesting and is one of the first paper to demonstrate circadian rhythms in both positive and negative moods at the population level. First, the authors show that social media (Twitter) can be a proxy for assessing mood  by demonstrating that behaviors that are known to follow a circadian rhythm (e.g., eating) are reflected in tweets with specific time-of-day peaks and troughs. That is, their analysis detected strong periodicity in daily activities, including meals, sleep times, or work patterns. More interestingly, the author's found that mood follows a circadian rhythm at the population-level. Based on specific tweeted words corresponding to "happiness", "fatigue", "anger", etc., the authors found mood typically starts positively in the morning and deteriorates throughout the day. They also establish that positive and negative moods fluctuate independently. Interestingly, they also show that anger and fatigue are unaffected across the year. 

#### Figure Analysis: Figure 1
For Dzogang et al.  **Figure 1**, they look at the variation of word volume within the 24-hour cycle, with a 99% confidence interval. In order to achieve this, author's averaged the hourly word volume. I then assume they calculated their 99% confidence intervals from student t-test. They have the y-axis representing that average volume per hour in the top 20,000 most frequent words across the 4 years. Although not labeled, the x-axis represents hours, starting at 6 AM in the morning. 

- This figure includes descriptive and inferential statistics (99% confidence intervals)

```{r, Fig 1}
# Start by loading in libraries
library(tidyverse)
library(dplyr)
library(ggplot2)
library(skimr)
library(reshape2)
library(zoo)
library(chron)


# Grab data
f <- "https://raw.githubusercontent.com/kierstenbell/data-analysis-rep/main/data/vol.csv"
d <- read.csv(f, sep = " ")
colnames(d) <- c("date", "time", "wordfreq") #creating column names to make data easier to work with

# Let's take a look out our data
head(d)
skim(d)

# There is date (01-01-2010 - 31-10-2014), time (00:00:00-midnight - 23:00:00-11 PM), and word frequency. 
# To generate the figure, we will need to average each hour across all the days and generate 99% confidence intervals

# Get average words per hour (AvgFrequency) and generate 99% confidence intervals (alpha = 0.01)
alpha = 0.01

d.Avg <- summarize(.data = d, 
                      AvgFrequency = mean(wordfreq),
                      lower_ci = mean(wordfreq) + qnorm(1 - (alpha/2))* sqrt(var(wordfreq)/length(wordfreq)), # lower 99% CI
                      upper_ci = mean(wordfreq) - qnorm(1 - (alpha/2))* sqrt(var(wordfreq)/length(wordfreq)), # upper 99% CI
                      .by = time)

# Author's plot starts at 6 (AM); current data will start at midnight.
# Mutate the data so that it can start at the same time (ie make 6 AM "01") 
# There's probably a better way to do this but I'm creating the variable "x" so that 6 AM will be first on the chart, followed by 7 AM, until 5 AM is the last point on the graph. It is possible to shift the time using as.POSIXct (see Figure 2) but I cannot get it to work.
d.Avg <- d.Avg %>%
 mutate(x = case_when(time =="00:00:00" ~ "19",
                      time =="01:00:00" ~ "20",
                      time =="02:00:00" ~ "21",
                      time =="03:00:00" ~ "22",
                      time =="04:00:00" ~ "23",
                      time =="05:00:00" ~ "24",
                      time =="06:00:00" ~ "01",
                      time =="07:00:00" ~ "02",
                      time =="08:00:00" ~ "03",
                      time =="09:00:00" ~ "04",
                      time =="10:00:00" ~ "05",
                      time =="11:00:00" ~ "06",
                      time =="12:00:00" ~ "07",
                      time =="13:00:00" ~ "08",
                      time =="14:00:00" ~ "09",
                      time =="15:00:00" ~ "10",
                      time =="16:00:00" ~ "11",
                      time =="17:00:00" ~ "12",
                      time =="18:00:00" ~ "13",
                      time =="19:00:00" ~ "14",
                      time =="20:00:00" ~ "15",
                      time =="21:00:00" ~ "16",
                      time =="22:00:00" ~ "17",
                      time =="23:00:00" ~ "18"))
d.Avg
                         
# Let's generate the plot
fig.1 <- ggplot(d.Avg, aes(x = x, y = AvgFrequency, group = 1)) + # group = 1 needed for geom_line()
      geom_point() + # Add points
      geom_line() + # Connect the points, **need group = 1 in aes 
      geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.4, fill = "blue") +  # Add shaded area for confidence interval; lower_ci and upper_ci calculated above
      scale_x_discrete(labels = c("6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "0", "1", "2", "3", "4", "5")) + # Manually change the x-axis labels to the correct times
      scale_y_continuous(limits = c(0.5e+05, 3e+05)) +  # Adjust y-axis limits
      labs(x = "Hour", y = "Average Word Frequency") +  # Label the axes
      ggtitle("Figure 1 Replicate")  # Add a title

# Compare generated plot with original figure
# Grab Fig 1 Url from Github
AuthorsFig1_url <- "https://github.com/kierstenbell/data-analysis-rep/blob/main/Figures/Figure%201.jpg?raw=true"  

# Output plot to a temporary files in order to use knitr
plot_file <- tempfile(fileext = ".png")
ggsave(plot_file, plot = fig.1, width = 5, height = 4, dpi = 300)

# Compare my figure (top) to author's figures (bottom)
knitr::include_graphics(c(plot_file, AuthorsFig1_url))
```

Figure 1 was able to be mostly successfully replicated. The authors did not report any physical numbers, so it is unclear whether this is an *exact* match; however, it is graphically very similar.  The data was easy to access (unlike Hughey & Butts 2016 paper!); however, the authors did not provide a lot of detail about how they conducted this particular analysis. I made some assumptions, but it appears as though those assumptions were correct. 

Based on this data, the UK population does not Tweet very much between 2 AM - 9 AM. Peak tweeting, as defined characterized by use of 20,000 most frequent words, occurs between 9 - 11 PM.  

#### Figure Analysis: Figure 2
For Dzogang et al.  **Figure 2**, they are showing that their methods on a group of example words (e.g., "sunrise" or "bus") are over-expressed and under-expressed at the expected times. In this case, "sunrise" and "bus" are overexpressed at 8 AM with some slight overexpression in the afternoon.  To demonstrate this, they use heatmaps that show the relative expression of a word in different months in each row and its relative expression during the 24-hours in each column. In other words, the y-axis is months and the x-axis is 24 hours. The authors now choose to start their x-axis at 0 (i.e., midnight), unlike Figure 1, for unknown reasons. The heatmaps are smoothed using cubic interpolation with seven interpolated points between each sampled value. Contours indicate sunrise and sunset in local time in London, United Kingdom. I was not able to replicate the cubic interpolation or plot the sunrise/sunset times onto the heat map.

This analysis is much more difficult and **can not be completed**.  In all their 9,000 csv files, there is not one called "sunrise" or "bus". Authors also do not describe what software they used or how they actually collected this data, so I am unable to attempt to glean Twitter data for these words. Authors also did not include sunrise/sunset data.  In order to attempt this, I will use one of the files they have included called "sunset". So although it will not be the same as what they have provided, it will be an attempt to mimic what they did. 

I collected the data from [here]("https://www.timeanddate.com/sun/uk/london?month=10&year=2010") that included that date and time of sunset in the UK. This information is uploaded to github. Authors should have included this instead of manually having to extract it 

Figures 3 - 11 are very similar. It is not clear exactly what words were considered for the plots. 

```{r, Figure 2}
# Load data set titled "sunset", where `sunset' was extracted based on the Linguistic Inquiry and Word Count.
f <- "https://raw.githubusercontent.com/kierstenbell/data-analysis-rep/main/data/sunset.csv"
d_sunset <- read.csv(f, sep = " ")
colnames(d_sunset) <- c("date", "time", "wordfreq") #creating column names to make data easier to work with

# Check to make sure data looks ok
# Note: certain hours are missing - this indicates that the word "sunset" was not tweeted
head(d_sunset)

# Preprocess the data
# as.POSIXct converts character class to the POSIXlt class which allows manipulation of date/time
# This will allow us to easily extract month data
d_sunset$date_time <- as.POSIXct(paste(d_sunset$date, d_sunset$time), format = "%d-%m-%Y %H:%M:%OS")
d_sunset$hour_of_day <- as.numeric(format(d_sunset$date_time, "%H"))
d_sunset$month <- as.numeric(format(d_sunset$date_time, "%m"))
d_sunset$year <- as.character(format(d_sunset$date_time, "%Y"))

# if we skim this data set, we will see that we have different classes
skim(d_sunset)

# Average the data across the 4 years
# Since we are averaging across the years, but needed to keep month and hour, use aggregate() instead of groupby()
d_sunset.Avg <- aggregate(x=d_sunset$wordfreq, 
                          by=list(d_sunset$hour_of_day,d_sunset$month), FUN=mean)

colnames(d_sunset.Avg) <- c("hour", "month", "wordfreq")
head(d_sunset.Avg)

# Create a matrix with hours of the day as columns (0 - 23) and months (1-12) as rows
sunset_df <- dcast(d_sunset.Avg, month ~ hour, value.var = "wordfreq")
sunset_matrix <- as.matrix(sunset_df) # matrix needed for heatmap in ggplot
sunset <- melt(sunset_matrix) # needed for heatmap in ggplot
head(sunset) # checking to make sure there are correct values

# Created my own csv sheet with sunset times in London, UK from 01-01-2010 - 31-10-2014
# Load data set titled "SunsetTimes"
g <- "https://raw.githubusercontent.com/kierstenbell/data-analysis-rep/main/data/SunsetTimes.csv"
d_sunset.times <- read_csv(g, col_names = TRUE)
head(d_sunset.times)

# Process data by converting characters to as.posixct so we can average the time 
d_sunset.times$date_time <- as.POSIXct(paste(d_sunset.times$Date, d_sunset.times$Sunset), format = "%Y-%m-%d %H:%M")

# Extract only the time ; if change to as.numeric throws an error 
d_sunset.times$time <- format(d_sunset.times$date_time, format="%H:%M:%S")

# Extract month
d_sunset.times$month <- as.character(format(d_sunset.times$date_time, "%m"))

# Extract year
d_sunset.times$year <- as.character(format(d_sunset.times$date_time, "%Y"))

# Get average sunset time 
library(chron)
d_sunset.times.average <- d_sunset.times %>%
  group_by(month) %>%
  reframe(AvgSunSet = mean(chron::times(time)))
d_sunset.times.average

# I cannot figure out how to convert the <time> variable to a <numeric> one 
# used timediff() as.positx() etc. I will just hardcode the tibble for now... 
# decimals were determined by dividing the minutes by 60; so 16:22 = 16 + 22/60= 16.37
month <- 1:12
AvgSunSet <- c(16.37, 17.22, 18.22, 19.95, 20.77, 21.28, 21.13, 20.31, 19.22, 17.97, 16.18, 15.88)
AvgSunSetTimes <- data.frame(month, AvgSunSet)

# Perform cubic interpolation
# Authors state they want "cubic interpolation" with 7 interpolated points, which would be a spline
# Authors had  interpolated points between each sample value so xout = 7
# this does not work, nor does ggplot accept this 
#interpolated_data <- apply(word_freq_matrix[, -1], 1, function(x) {
#  na.approx(x, xout = 7, rule = 2, method = "cubic", ties="ordered")  # specify the ties argument
# })

# Create a heatmap, adding geom_point() with sunset data did nothing
heatmap <- ggplot(data = sunset, aes(x = Var2, y = Var1, fill = value)) +
                  geom_tile() +
                  scale_fill_gradientn(colours = c("green", "white", "red")) +
                  coord_fixed(ratio = 1) +  # Adjust ratio_value to lengthen the plot
                  labs(x = "Hour of Day", y = "Month", fill = "Word Frequency") +
                  scale_y_continuous(breaks = seq(1, 12, by = 1)) +
                  theme(axis.text.x = element_text(angle = 45, hjust = 1))
heatmap

# Sunset data
sunset <- ggplot(data = AvgSunSetTimes, aes(x = AvgSunSet, y = month, group = 1), color = "black", size = 100) +
          geom_point() +
         # geom_line() + this connects points based on the x-values, not y values... 
          xlim(0, 23) + 
          scale_x_continuous(breaks = seq(0, 23, by = 1)) +
          scale_y_continuous(breaks = seq(1, 12, by = 1)) 

sunset

## Hypothetically these should be overlaid, but errors are thrown. However, you can see that the pattern in the heatmap reflects the timing of the sunset. Although the authors show "sunrise" and "bus", we see that the same pattern emerges with "sunset"! Note: author's month scale is switched

# Compare generated plot with original figure
# Grab Fig 2 Url from Github
# Compare my figure (top) to author's figures (bottom)
knitr::include_graphics("https://github.com/kierstenbell/data-analysis-rep/blob/main/Figures/Figure%202.jpg?raw=true")

```

### Reflection
This assignment was incredibly difficult. Although this paper was a lot easier to handle than the Hughey and Buttes paper, it was still quite the challenge. The authors made *most of their data available, despite making it seem like all of their data was. They should've included data like sunrise/sunset times (which I manually collected), as well as the data sets for "bus" and "sunrise" for which they plotted.  In general, the authors of this paper should have included a lot more information about their specific data sets. I was not able to replicate Figures 3-11 (not shown) as I was not sure what csv files were categorized as which moods. Considering that their data was already in a supplemental file, it is odd to me that they didn't include this information. Additionally, it would've been helpful (for me) if the authors included some descriptive statistics for Figure 1 so that I knew whether or not my replication was correct. 

For the Figures that were not shown, the authors completed analysis that I did not understand. While I am familiar with sine fitting, I struggled to figure out how to interpolate cubic plots with 7 sample points. I would need more training in statistics and circadian mathematics before I would be able to reconstruct those plots properly. 

Lastly, even if I did have the "sunrise"/"bus" data set for Figure 2, I would not able to replicate Figure because I could not figure out how to include the contour lines on top of the heatmap. I received a variety of errors, and I would need to spend additional time to finally be able to include the sunset/sunrise times on the heatmaps of specific words.  

Overall, I have a lot more appreciation for circadian rhythm researchers. The data wrangling and statistical analysis are not easy. 